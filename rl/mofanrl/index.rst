莫烦python强化学习总结
======================

Reference
---------

-  https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/

简介
----

什么是强化学习？
~~~~~~~~~~~~~~~~

-  定义与原理：强化学习能让机器在环境中通过不断试错、累积经验来拿到高分，表现出优秀成绩.
   就像有一位虚拟老师，老师只给行为打分，计算机通过记住高分和低分对应的行为，来学会选择能带来高分的行为，避免低分行为，具有分数导向性.
-  与监督学习的对比：监督学习有已有的数据和对应的正确标签，而强化学习一开始没有数据和标签，需要通过在环境中的一次次尝试来获取数据和标签，然后学习数据与标签的对应规律.
-  强化学习算法：强化学习包含多种算法，如通过行为价值选取特定行为的
   Q-learning、sarsa、DQN，直接输出行为的 policy
   gradients，以及了解所处环境并从虚拟环境中学习的算法等.

强化学习方法汇总
~~~~~~~~~~~~~~~~

强化学习是机器学习中一个发展历史较长的大家族，包含多种方法，以下从分类角度对其进行总结：

1. **Modelfree 和 Modelbased**\ ： 

   - **Modelfree**\ ：不尝试理解环境，直接接受环境反馈，如Q-learning、Sarsa、Policy Gradients等，机器人在这种方式下在真实世界行动，缺乏对环境的预先认知，行动后果较为直接且可能代价较大. 

   - **Modelbased**\ ：通过过往经验理解真实世界并建立模型模拟现实世界反馈，不仅能在现实世界行动，也能在模拟世界行动. 其优势在于具有想象力，能预判断未来情况并选择最优策略，例如AlphaGo在围棋场上超越人类就借助了这一特性. 可以看作是在Modelfree基础上多了为真实世界建模以及在虚拟环境行动的环节. 

2. **基于概率和基于价值**\ ： 

   - **基于概率**\ ：通过感官分析环境，直接输出下一步各种动作的概率并依概率行动，每个动作都有被选中的可能性，如Policy Gradients. 其优点是可在连续动作中选取特定动作.  -

   - **基于价值**\ ：输出所有动作的价值，依据最高价值选择动作，决策较为确定，如Q-learning、Sarsa等. 但对于选取连续动作无能为力. 还可将两者结合形成Actor-Critic方法，actor基于概率做动作，critic对动作给出价值，加速学习过程.  3.
   
3. **回合更新和单步更新**\ ： 

   -  **回合更新**\ ：以游戏为例，需等待游戏结束，总结该回合所有转折点后更新行为准则，如Monte-carlo learning和基础版的policy gradients等.  
   
   - **单步更新**\ ：在游戏进行中每一步都更新，无需等待游戏结束，边玩边学习，效率更高，目前大多方法基于此，如Q-learning、Sarsa、升级版的policy gradients等.  

4. **在线学习和离线学习**\ ： -
   
   - **在线学习**\ ：学习者必须本人在场，边玩边学习，如Sarsa和Sarsa lambda. 

   - **离线学习**\ ：学习者可自己玩，也可通过观察别人玩来学习行为准则，从过往任何人的经验中学习，不必实时在线学习，如Q-learning以及在此基础上发展出的让计算机学会玩电动的Deep - Q - Network. 
