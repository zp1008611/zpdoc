# 时序差分（TD）学习

## Reference

- https://people.cs.umass.edu/~bsilva/courses/CMPSCI_687/Fall2022/Lecture_Notes_v1.0_687_F22.pdf
- https://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/slides/lec21.pdf



时序差分学习由（Sutton，1988a）提出，是一种策略评估算法. 与蒙特卡罗算法一样，它通过采样（使用策略 $\pi$ 选择动作并观察结果）从经验中学习，而无需了解 $p$ 和 $R$ . 然而，与动态规划方法类似，它基于其他估计值来生成估计值，即进行自举. 后一个特性意味着它可以在一个情节结束之前执行其更新（这是蒙特卡罗方法的要求 ）. 

与之前的算法一样，TD从一个初始值函数估计 $v$ 开始. 作为一种评估算法，而不是控制算法，它估计 $v^{\pi}$（而不是通过估计 $q^{*}$ 来获得 $\pi^{*}$ ）. 给定智能体处于状态 $s$ ，采取动作 $a$ ，转移到状态 $s'$ 并获得奖励 $r$ ，TD更新为：
$$
v(s) \leftarrow v(s) + \alpha(r + \gamma v(s') - v(s)).
$$  

用其他符号表示，它可以等价地定义为：
$$
v(S_t) \leftarrow v(S_t) + \alpha(R_t + \gamma v(S_{t + 1}) - v(S_t)).
$$  

这与5.2节中基于梯度的蒙特卡罗算法非常相似，只是它使用 $R_t + \gamma v(S_{t + 1})$ 作为目标，而不是 $G_t$ . 

时序差分误差（TD误差）$\delta_t$ 定义为：
$$
\delta_t = R_t + \gamma v(S_{t + 1}) - v(S_t),
$$  
这使我们可以将TD更新写为：
$$
v(S_t) \leftarrow v(S_t) + \alpha \delta_t.
$$  

注意，正的TD误差意味着观察到的结果（奖励 $R_t$ 加上所得状态的价值 $v(S_{t + 1})$ ）比预期的要好（即当前状态的价值 $v(S_t)$ ）. 还要注意，TD误差可以涉及不同的项：它可以使用当前的值估计 $v$ ，也可以使用真实的状态 - 值函数 $v^{\pi}$ . 在这两种情况下，$\delta_t$ 都称为TD误差. 



进一步考虑正TD误差的所有可能原因. 正的TD误差可能出现是因为：1）$v(s)$ 太小；2）奖励和状态转移的随机性结合了运气因素；和/或 3）$v(s')$ 太大. 如果 $v = v^{\pi}$ ，那么TD误差是由于原因2造成的，但平均来说是零均值更新（这遵循贝尔曼方程 ）. 如果 $v \neq v^{\pi}$ ，那么TD更新试图纠正原因1，但不会对原因2和3进行修正. 这是因为，根据马尔可夫性质，我们知道 $v^{\pi}(s')$ 不依赖于我们如何到达状态 $s'$ ，然而TD误差是由于转移 $(s, a, r, s')$ 产生的，即TD误差描述了到达 $s'$ 之前的事件，不应该影响我们对状态 $s'$ 价值的估计. 

注意，TD更新也可以看作是将贝尔曼方程转换为更新规则，就像我们在策略评估中使用动态规划一样. 然而，使用动态规划（因为我们假设 $p$ 和 $R$ 是已知的）时，我们可以精确计算贝尔曼方程的右边，而TD算法不假设 $p$ 和 $R$ 是已知的，所以使用采样 —— 它根据 $\pi$ 、$p$ 和 $R$ 对 $A_t$ 、$R_t$ 和 $S_{t + 1}$ 进行采样. 

注意，我们可以将带有函数近似的TD更新写为：
$$
w \leftarrow w + \alpha (R_t + \gamma v_w(S_{t + 1}) - v_w(S_t)) \frac{\partial v_w(S_t)}{\partial w}.
$$  


有人可能会认为TD算法是一种梯度算法，与5.2节中基于梯度的蒙特卡罗算法非常相似，只是目标替换为了 $R_t + \gamma v(S_{t + 1})$ . 然而，事实并非如此. 考虑如何将TD推导为一种梯度算法. 我们从定义损失函数开始：
$$
\begin{align*}
L(w) &= \mathbb{E}\left[\frac{1}{2}(R_t + \gamma v_w(S_{t + 1}) - v_w(S_t))^2\right] &(228)\\
&= \mathbb{E}\left[\frac{1}{2}\delta_t^2\right]. &(229)
\end{align*}
$$

然后我们计算梯度：
$$
\begin{align*}
\frac{\partial}{\partial w} L(w) &= \frac{\partial}{\partial w} \mathbb{E}\left[\frac{1}{2}(R_t + \gamma v_w(S_{t + 1}) - v_w(S_t))^2\right] &(230)\\
&= \mathbb{E}\left[\delta_t \left(\gamma \frac{\partial v_w(S_{t + 1})}{\partial w} - \frac{\partial v_w(S_t)}{\partial w}\right)\right] &(231)\\
&= \mathbb{E}\left[-\delta_t \left(\frac{\partial v_w(S_t)}{\partial w} - \gamma \frac{\partial v_w(S_{t + 1})}{\partial w}\right)\right], &(232)
\end{align*}
$$
其中最后一项的符号变化是为了得到标准形式. 这表明了一种随机梯度下降更新（注意这里的负号是因为这是一个下降算法，负号抵消了 $\delta_t$ 之前的负号）：
$$
w \leftarrow w + \alpha \delta_t \left(\frac{\partial v_w(S_t)}{\partial w} - \gamma \frac{\partial v_w(S_{t + 1})}{\partial w}\right).
$$  

注意，我们开始使用的损失函数是 $L(w) = \mathbb{E}[\delta_t^2 / 2]$ . 这实际上不是我们想要的目标！由于 $R_t$ 和 $S_{t + 1}$ 的随机性，即使我们的估计器 $v_w$ 完全正确（等于 $v^{\pi}$ ），TD误差也不为零. 因此，期望TD误差为零. 当值估计正确时，期望平方TD误差不为零. 最小化期望平方TD误差不会得到状态 - 值函数，而是最小化期望平方TD误差（这个状态 - 值函数的期望误差称为均方贝尔曼误差MSBE）. 稍后我们会回过头来重新考虑对期望平方误差求导. 现在，让我们继续分析用于最小化期望平方TD误差的更新. 

考虑当TD误差为正时这个更新会做什么：它改变 $w$ 以增加 $v_w(S_t)$ 并减少 $v_w(S_{t + 1})$ ，而TD更新只增加 $v_w(S_t)$ . 为了更清楚地说明这一点，注意(233) 使用表格函数近似可以写为：
$$
v(S_t) \leftarrow v(S_t) + \alpha \delta_t
$$  
$$
v(S_{t + 1}) \leftarrow v(S_{t + 1}) - \alpha \gamma \delta_t.
$$  

这个替代算法不是残差梯度（Baird，1995），但很相似. 

仅仅因为我们试图将TD算法推导为损失函数的梯度并得到了不同的算法，并不意味着TD算法不是梯度算法，它只是意味着它不是（关于）$L$ 的随机梯度算法. 然而，它可以被证明对于任何目标函数，TD算法都不是随机梯度算法. 如果是这样，那么期望TD更新必须是损失函数（目标函数的梯度）的梯度. 即，
$$
\mathbb{E}\left[\delta_t \frac{\partial v_w(S_t)}{\partial w}\right],
$$  
将是一个函数的梯度. 我们可以证明情况并非如此：(236) 不是任何损失函数（具有连续二阶导数）的梯度. 更准确地说，回想一下对于任何具有连续二阶偏导数的函数 $L$ ，在 $w$ 处的海森矩阵 $\partial^2 L(w) / \partial w^2$ 必须是对称的（见施瓦茨定理 ）. 如果(236) 是函数的梯度，那么它的导数将是海森矩阵. 与其计算完整的导数，让我们计算关于 $w_j$ 的(236) 的偏导数，即(236) 中第 $j$ 个元素：
$$
\begin{align*}
\frac{\partial}{\partial w_i} \delta_t \frac{\partial v_w(S_t)}{\partial w_j} &= \delta_t \frac{\partial^2 v_w(S_t)}{\partial w_i \partial w_j} + \frac{\partial v_w(S_t)}{\partial w_j} \frac{\partial}{\partial w_i} (R_t + \gamma v_w(S_{t + 1}) - v(S_t)) &(237)\\
&= \delta_t \underbrace{\frac{\partial^2 v_w(S_t)}{\partial w_i \partial w_j}}_{(a)} + \underbrace{\frac{\partial v_w(S_t)}{\partial w_j} \left(\gamma \frac{\partial v_w(S_{t + 1})}{\partial w_i} - \frac{\partial v(S_t)}{\partial w_i}\right)}_{(b)}. &(238)
\end{align*}
$$

注意，虽然项 (a) 是对称的（如果 $v_w$ 具有连续二阶导数，交换 $i$ 和 $j$ 其值不变），但项 (b) 不是对称的 —— 交换 $i$ 和 $j$ 会改变其值. 要明白为什么，考虑使用表格函数近似的情况，其中 $w_j$ 是 $S_t$ 的权重，$w_i$ 是 $S_{t + 1}$ 的权重，且 $S_t \neq S_{t + 1}$ ，项 (b) 不一定为零，但如果 $w_j$ 是 $S_{t + 1}$ 的权重，那么该项将为零. 因此，期望TD更新的导数不是对称的，所以TD更新不能是具有连续二阶偏导数的损失函数的随机梯度更新. 

尽管TD算法不是梯度算法，但它具有理想的收敛性质. 当使用表格表示进行值函数近似时，在标准假设和衰减步长下（Dayan和Sejnowski，1994；Jaakkola等人，1994），TD几乎必然收敛到 $v^{\pi}$ ；如果步长足够小，它在均值上收敛到 $v^{\pi}$（Sutton，1988b）. 当使用线性函数近似时（当对于某个 $n$ ，$v_w(s)$ 可以写为 $w^T \phi(s)$ ，其中 $\phi : \mathcal{S} \to \mathbb{R}^n$ ），在标准假设下，TD几乎必然收敛到某个权重向量 $w_{\infty}$（Tsitsiklis和Van Roy，1997）. 如果存在一个权重向量使得 $v_w = v^{\pi}$ ，那么 $w_{\infty} = v^{\pi}$ ，TD将收敛到使 $v_w$ 为 $v^{\pi}$ 的权重. 然而，如果不存在这样的权重向量 $w$ 使得 $v_w = v^{\pi}$（如果给定 $v_w$ 对于不同 $w$ 所能产生的函数类，状态 - 值函数不能被精确表示 ），那么TD收敛到的权重向量（几乎必然）不一定是“最佳”的可能权重向量 $w^{*}$ ：
$$
w^{*} \in \underset{w}{\arg\min} \mathbb{E}[(v_w(S_t) - v^{\pi}(S_t))^2].
$$ 

然而，$w_{\infty}$ 满足以下不等式，确保TD几乎必然收敛到的权重不会“离”这些最优权重“太远”（Tsitsiklis和Van Roy，1997，定理1）：
$$
\mathbb{E}[(v_{w_{\infty}}(S_t) - v^{\pi}(S_t))^2] \leq \frac{1}{1 - \gamma} \mathbb{E}[(v_{w^{*}}(S_t) - v^{\pi}(S_t))^2].
$$  

当使用非线性函数近似时，TD可能会发散. 

什么是更好的目标，蒙特卡罗回报 $G_t$ ，还是TD使用的目标 $R_t + \gamma v(S_{t + 1})$ ？ 每一个都是 $v^{\pi}(S_t)$ 的估计器. 均方误差（MSE）是衡量一个估计器有多“差”的常用指标. 设随机变量 $X$ 是 $\theta \in \mathbb{R}$ 的估计器. $X$ 的MSE定义为：
$$
MSE(X) := \mathbb{E}[(X - \theta)^2].
$$  

MSE可以分解为两个分量：平方偏差和方差：
$$
MSE(X) = Bias(X)^2 + Var(X),
$$  
其中 $Bias(X) = \mathbb{E}[X - \theta]$ ，$Var(X)$ 是 $X$ 的方差. 再次考虑两个可能的目标，每一个都是 $v^{\pi}$ 的估计器，哪一个是“更好”的估计器？

蒙特卡罗回报是无偏的，所以它的偏差为零. 然而，它通常具有高方差，因为它取决于一个情节中出现的所有奖励. 如果 $v \neq v^{\pi}$ ，TD目标可能是有偏的，因为它替换了蒙特卡罗回报中的所有奖励，除了第一个奖励，并且可能有一个有偏的估计值，$v(S_{t + 1})$（这是因为 $v \neq v^{\pi}$ ）. 然而，它可以具有低得多的方差，因为它只向前看一个时间步：$R_t$ 和 $S_{t + 1}$（TD目标中仅有的随机项）都可以在单个时间步后计算. 因此，TD目标与蒙特卡罗目标形成对比：蒙特卡罗目标具有高方差但无偏差，TD目标具有低方差但高偏差. 稍后我们将讨论创建估计器的方法，这些估计器可以在偏差和方差之间提供更好的权衡，以获得“更好”的 $v^{\pi}$ 估计值.  

## 函数近似
在继续之前，值得更详细地讨论函数近似. 首先，注意如果 $v_w$ 是 $w$ 的线性函数，我们就称 $v_w$ 是一个线性函数近似器. 这并不意味着 $v_w$ 一定是状态的线性函数. 此外，我们通常将线性函数近似器写为：
$$ 
v_w(s) = w^T \phi(s),
$$  
其中 $\phi : \mathcal{S} \to \mathbb{R}^n$ 将状态映射为特征向量. 

$\phi$ 的一种可能选择是多项式基，它假定 $s$ 是实值的（它可以扩展到多元多项式基，在这种情况下 $s$ 是实向量 ）. 大多数基都有一个我们称之为阶数的参数，它控制将产生多少个特征. $k$ 阶多项式基为：
$$
\phi(s) = \begin{bmatrix}1 \\ s \\ s^2 \\ \vdots \\ s^k\end{bmatrix}.$$  

根据斯通 - 魏尔斯特拉斯定理，对于任何连续函数，只要阶数足够高，就可以以任意期望的精度进行近似. 

用于值函数近似的傅里叶基是一种常见的线性函数近似器，对于大多数标准的强化学习基准问题都非常有效. 介绍傅里叶基的论文（Konidaris等人，2011b）在此时应该很容易读懂，可以在此处找到. 请阅读该论文. 注意，在应用多元傅里叶基之前，状态应该进行归一化.  

## 马尔可夫决策过程（MDP）的最大似然模型与时序差分学习
如果我们有关于许多状态转移 $(s, a, r, s')$ 的数据，我们可以用这些数据来估计 $p$ 和 $d_R$ . 注意，无论这些数据是如何生成的（通过运行完整的情节、随机采样状态等 ），我们都可以这样做. 目前，我们假设 $a$ 是根据 $\pi$ 进行采样的. 一种常见的估计器是最大似然模型，即估计 $p$ 和 $d_R$ ，使得我们看到现有数据的概率最大化. 对于具有有限状态和动作的MDP，其最大似然模型正是我们所预期的那样. 即：
$$
\hat{P}(s, a, s') = \frac{\#(s, a, s')}{\#(s, a)}
$$  
$$
\hat{R}(s, a) = \text{mean}(r|s, a),
$$  
其中 $\#(s, a, s')$ 是数据中 $(s, a, s')$ 出现的次数，$\#(s, a)$ 是在状态 $s$ 采取动作 $a$ 的次数，$\text{mean}(r|s, a)$ 是在这些样本中 $r$ 的平均值. 

一旦我们得到估计值 $\hat{P}$ 和 $\hat{R}$ ，如果它们是真实的转移函数和奖励函数，我们就可以使用动态规划评估方法来求解 $v^{\pi}$ . 一个有趣的问题是：这个值函数估计与在相同数据上运行TD算法所产生的值函数估计相比如何？ TD是否会收敛到与构建最大似然模型并求解 $\pi$ 的值函数时完全相同的估计值（如果每个状态至少被观察到一次或更多次 ）？ 也许令人惊讶的是，TD确实收敛到这个值函数估计. 所以，人们可能会认为TD是一种有效的方法，用于计算如果构建最大似然模型并求解 $\pi$ 的值函数时会得到的值函数. 

在实际应用中，TD会有用得多. 注意，估计模型至少需要存储 $|S|^2|A|$ 个数字，而TD只需要存储 $|S|$ 个数字. 此外，当状态是连续的时，估计 $p$（和 $R$ ）很困难，而使用函数近似器（如神经网络 ）估计值函数则很直接. 这是因为 $p$ 是一种分布，所以估计 $p$ 不仅仅是回归问题，还需要进行密度估计. 

## Sarsa：将TD用于控制
思路：我们可以使用时序差分（TD）来估计 $q^{\pi}$ ，同时使策略 $\pi$ 关于 $q^{\pi}$ （近似）贪心. 首先，我们必须确定如何使用TD来估计动作 - 值函数，而不是状态 - 值函数. 对于给定的状态转移 $(s, a, r, s', a')$ ，表格形式的 $q$ 的TD更新为：
$$
q(s, a) \leftarrow q(s, a) + \alpha(r + \gamma q(s', a') - q(s, a)),
$$  
使用任意函数近似时，$q_w$ 的TD更新为：
$$
w \leftarrow w + \alpha (r + \gamma q_w(s', a') - q_w(s, a)) \frac{\partial q_w(s, a)}{\partial w}.
$$ 

我们将 $r + \gamma q(s', a') - q(s, a)$ 这一项称为TD误差. 不过，当有人提到TD误差时，通常指的是使用状态 - 值函数计算的TD误差. 

从（使用动作 - 值函数的）TD误差角度，并用状态、动作和奖励的随机变量表示，使用任意函数近似的TD更新可写为：
$$
\delta_t = R_t + \gamma q_w(S_{t + 1}, A_{t + 1}) - q_w(S_t, A_t)$$  
$$
w_{t + 1} \leftarrow w_t + \alpha \delta_t \frac{\partial q_w(S_t, A_t)}{\partial w}.
$$  

可以将动作 - 值函数的TD更新看作是在一个不同的马尔可夫决策过程（MDP）中状态 - 值函数的TD更新，这个不同的MDP的状态被扩展为包含根据要评估的策略所选择的动作. 也就是说，考虑一个MDP $M$ . 我们可以构造一个新的MDP $M'$ ，其状态为 $x = (s, a)$ ，其中 $s$ 是 $M$ 中的状态，$a$ 是 $M$ 中的动作. $M'$ 的转移函数使得 $s$ 的转移与在 $M$ 中一样，并根据要评估的策略 $\pi$ 选择动作 $a$ . $M'$ 中的动作并不重要，我们假设 $|\mathcal{A}| = 1$ ，这样就只有一个动作. 如果我们对这个新MDP应用TD来估计 $v(x)$ （这里只有一个策略，所以省略策略上标 ），我们得到：
$$
v(x) = \mathbb{E}[G_t | X_t = x]
$$  
$$ 
[\text{对于 } M] = \mathbb{E}[G_t | S_t = s, A_t = a]
$$  
$$
[\text{对于 } M] = q^{\pi}(s, a).
$$  
其中 $X_t$ 是 $M'$ 在时刻 $t$ 的状态. 此外，用状态 $x$ 写出 $M'$ 的TD更新，我们就得到了(247) 中 $q$ 的TD更新. 因此，将TD应用于学习动作 - 值函数，等同于将TD应用于学习不同MDP的状态 - 值函数，所以它具有完全相同的收敛性质. 

现在我们可以使用TD算法来估计 $q^{\pi}$ ，然后关于 $q^{\pi}$ 进行贪心动作选择. 这可以看作是值迭代或广义策略迭代算法的一种近似形式. 这个算法被称为Sarsa，因为用于更新的数据是 $(s, a, r, s', a')$ . 表格形式的Sarsa伪代码如算法9所示，使用任意函数近似的Sarsa伪代码如算法10所示. 注意，我们不需要存储 $q(s_{\infty}, a)$ 的估计值，我们知道它为零，无需学习这个值. 

**算法9：表格形式的Sarsa**
1. 任意初始化 $q(s, a)$ ；
2. 对于每个情节执行以下操作
3. $s \sim d_0$ ；
4. 根据 $q$ 导出的策略（如 $\epsilon$-贪心策略或softmax策略）从 $s$ 选择动作 $a$ ；
5. 对于每个时间步，直到 $s$ 是终止吸收状态执行以下操作
6. 执行动作 $a$ 并观察 $r$ 和 $s'$ ；
7. 根据 $q$ 导出的策略从 $s'$ 选择动作 $a'$ ；
8. $q(s, a) \leftarrow q(s, a) + \alpha (r + \gamma q(s', a') - q(s, a))$ ；
9. $s \leftarrow s'$ ；
10. $a \leftarrow a'$ ；

**算法10：Sarsa**
1. 任意初始化 $w$ ；
2. 对于每个情节执行以下操作
3. $s \sim d_0$ ；
4. 根据 $q$ 导出的策略（如 $\epsilon$-贪心策略或softmax策略）从 $s$ 选择动作 $a$ ；
5. 对于每个时间步，直到 $s$ 是终止吸收状态执行以下操作
6. 执行动作 $a$ 并观察 $r$ 和 $s'$ ；
7. 根据 $q$ 导出的策略从 $s'$ 选择动作 $a'$ ；
8. $w \leftarrow w + \alpha (r + \gamma q_w(s', a') - q_w(s, a)) \frac{\partial q_w(s, a)}{\partial w}$ ；
9. $s \leftarrow s'$ ；
10. $a \leftarrow a'$ ；

为了保证Sarsa几乎必然收敛到最优动作 - 值函数，我们需要标准假设（有限状态、有限动作、有界奖励、$\gamma \in [0, 1)$ 、步长适当衰减 ），以及另外两个假设. 首先，所有状态 - 动作对必须被无限次访问. 其次，策略在极限情况下必须收敛到贪心策略（例如，$\epsilon_t = \frac{1}{t}$ ）. 这两个额外假设有时被称为GLIE假设：具有无限探索的极限贪心. 

## Q学习：异策略TD控制
Sarsa更新可被视为将贝尔曼方程转化为更新规则，而Q学习可被视为将贝尔曼最优性方程转化为更新规则. 基于状态转移 $(s, a, r, s')$ 的Q学习更新（出自Watkins, 1989）为：
$$
q(s, a) = q(s, a) + \alpha (r + \gamma \max_{a' \in \mathcal{A}} q(s', a') - q(s, a)),
$$  
使用任意函数近似时为：
$$
w = w + \alpha (r + \gamma \max_{a' \in \mathcal{A}} q_w(s', a') - q_w(s, a)) \frac{\partial q_w(s, a)}{\partial w}.
$$  

因此，Sarsa在每一步都朝着 $q^{\pi}$ 的估计值更新 $q$（其中 $\pi$ 是生成动作的策略），而Q学习在每一步都朝着 $q^{*}$ 的估计值更新 $q$，无论用于生成动作的是何种策略. 从这个意义上说，它是异策略的 —— 它估计 $q^{*}$ ，而不考虑生成数据的策略. 注意，Q学习在采样到 $S_{t + 1}$ 后即可进行更新，无需等待采样到 $A_{t + 1}$，而Sarsa必须等待直到 $A_{t + 1}$ 被采样. 在标准假设下，如果所有 $(s, a)$ 对都被无限次访问，Q学习会收敛到 $q^{*}$ . 这意味着Q学习不需要GLIE假设，采样策略无需在极限情况下变得贪心. 

由于Q学习不需要 $A_{t + 1}$ 来进行更新，其伪代码与Sarsa不同，只是更新中的 $q$ 有所修改. Q学习的伪代码如算法11和算法12所示. 

**算法11：表格形式的Q学习**
1. 任意初始化 $q(s, a)$ ；
2. 对于每个情节执行以下操作
3. $s \sim d_0$ ；
4. 对于每个时间步，直到 $s$ 是终止吸收状态执行以下操作
5. 根据 $q$ 导出的策略从 $s$ 选择动作 $a$ ；
6. 执行动作 $a$ 并观察 $r$ 和 $s'$ ；
7. $q(s, a) \leftarrow q(s, a) + \alpha (r + \gamma \max_{a' \in \mathcal{A}} q(s', a') - q(s, a))$ ；
8. $s \leftarrow s'$ ；

**算法12：Q学习**
1. 任意初始化 $w$ ；
2. 对于每个情节执行以下操作
3. $s \sim d_0$ ；
4. 对于每个时间步，直到 $s$ 是终止吸收状态执行以下操作
5. 根据 $q$ 导出的策略从 $s$ 选择动作 $a$ ；
6. 执行动作 $a$ 并观察 $r$ 和 $s'$ ；
7. $w \leftarrow w + \alpha (r + \gamma \max_{a' \in \mathcal{A}} q_w(s', a') - q_w(s, a)) \frac{\partial q_w(s, a)}{\partial w}$ ；
8. $s \leftarrow s'$ ；

TD、Sarsa和Q学习的收敛性质见下表.  

| | TD | Sarsa | Q - learning |
| ---- | ---- | ---- | ---- |
| 表格形式 | 收敛到 $v^{\pi}$（Tsitsiklis和Van Roy，1997） | 收敛到 $q^{\pi}$（Singh等人，2000） | 收敛到 $q^{*}$（Watkins和Dayan，1992；Tsitsiklis，1994） |
| 线性 | 收敛到一个与 $v^{\pi}$ “接近” 的策略（Tsitsiklis和Van Roy，1997） | 收敛（Perkins和Precup，2003） | 可能发散（Wiering，2004；Baird，1995） |
| 非线性 | 可能发散 | 可能发散 | 可能发散 |

**表1**：TD、Sarsa和Q学习的收敛性质. 有关完整细节（包括收敛类型和必要假设），请参阅提供的参考文献. 这些参考文献并非首次证明.  